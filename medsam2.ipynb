{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, pdb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch.nn import MSELoss\n",
    "from transformers import SamModel \n",
    "from transformers import SamProcessor\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from torch.optim import Adam\n",
    "import monai\n",
    "from monai.transforms import ToTensor\n",
    "\n",
    "from torchvision.transforms import ToPILImage, Compose, Resize, Grayscale, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/sddata/data/MedSAM/public_test_data_01_10_2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example = dataset[0]\n",
    "image = example[\"image\"]\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = example['label']\n",
    "label = np.array(label) * 255\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(label)[60:180,30:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        # color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    print(h, w)\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "axes.imshow(np.array(image))\n",
    "ground_truth_seg = np.array(example[\"label\"])\n",
    "show_mask(ground_truth_seg, axes)\n",
    "axes.title.set_text(f\"Ground truth mask\")\n",
    "axes.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch Dataset\n",
    "\n",
    "Below we define a regular PyTorch dataset, which gives us examples of the data prepared in the format for the model. Each example consists of:\n",
    "\n",
    "* pixel values (which is the image prepared for the model)\n",
    "* a prompt in the form of a bounding box\n",
    "* a ground truth segmentation mask.\n",
    "The function below defines how to get a bounding box prompt based on the ground truth segmentation. This was taken from here.\n",
    "\n",
    "Note that SAM is always trained using certain \"prompts\", which you could be bounding boxes, points, text, or rudimentary masks. The model is then trained to output the appropriate mask given the image + prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  # add perturbation to bounding box coordinates\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "        # get bounding box prompt\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "        # prepare image and prompt for the model\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "        # add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Hyperparameter tuning could improve performance here\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "epoch_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    batch_epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "        \n",
    "        # compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "        \n",
    "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        batch_epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(batch_epoch_losses)}')\n",
    "    epoch_losses.append(mean(batch_epoch_losses))\n",
    "    plt.figure()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"/sddata/data/MedSAM/model/facebook_sam-vit-base_BB.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "raw_image = Image.open('/sddata/data/MedSAM/public_test_data_01_10_2023/Color_Fundus/visivite_GA.png')\n",
    "# Convert RGBA image to RGB\n",
    "raw_image = raw_image.convert(\"RGB\")\n",
    "\n",
    "# Label\n",
    "label = Image.open('/sddata/data/MedSAM/public_test_data_01_10_2023/labels/visivite_GA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(raw_image))\n",
    "print(type(label))\n",
    "print(np.array(raw_image).shape)\n",
    "print(np.array(label).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.array(label)[:,:,0] == 0).all()) # R\n",
    "print((np.array(label)[:,:,1] == 0).all()) # G\n",
    "print((np.array(label)[:,:,2] == 0).all()) # B\n",
    "print((np.array(label)[:,:,3] == 0).all()) # A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(label)[800:900,800:900,0] # R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array(label).shape[0]\n",
    "h = np.array(label).shape[1]\n",
    "ground_truth_seg = np.array(label)[:,:,0].reshape(w, h)\n",
    "ground_truth_seg_norm = ground_truth_seg / 255\n",
    "print(f\"{w} {h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "\n",
    "axes.imshow(np.array(raw_image))\n",
    "show_mask(ground_truth_seg_norm, axes, random_color=True)\n",
    "axes.title.set_text(f\"Ground truth mask\")\n",
    "axes.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, is_grayscale=False):\n",
    "        if isinstance(dataset, str):\n",
    "            self.dataset = pd.read_csv(os.path.join(dataset))\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.is_grayscale = is_grayscale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _load_image(self, image_path, is_grayscale=True, resize=(256, 256)):\n",
    "        image = Image.open(os.path.join(DATA_DIR,image_path))\n",
    "        image = image.convert('RGB')\n",
    "        # image = image.convert('RGB')\n",
    "        img_transform = Compose([\n",
    "            Resize(resize),\n",
    "            Grayscale() if is_grayscale else Lambda(lambda x: x),\n",
    "            # ToTensor() # it will convert to tensor and scale between 0-1\n",
    "            Lambda(lambda x: torch.from_numpy(np.array(x))),\n",
    "            \n",
    "        ])\n",
    "        image = img_transform(image)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(self.dataset, pd.DataFrame):\n",
    "            item = self.dataset.iloc[idx]\n",
    "            image = item['image_path_orig']\n",
    "            label = item['labels']\n",
    "            image = self._load_image(image, is_grayscale=False, resize=(256, 256))\n",
    "            label = self._load_image(label, is_grayscale=True, resize=(256, 256))\n",
    "            label = (label > 0).float()\n",
    "        else:            \n",
    "            item = self.dataset[idx]\n",
    "            image = item[\"image\"]\n",
    "            label = np.array(item[\"label\"])\n",
    "\n",
    "        # get bounding box prompt\n",
    "        # print(image)\n",
    "        # print(type(image))\n",
    "        # print(label.shape)\n",
    "        prompt = get_bounding_box(label.squeeze())\n",
    "\n",
    "        # prepare image and prompt for the model\n",
    "        print(image.min(), image.max())\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "        # print(image.min(), image.max())\n",
    "\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "        # inputs = {'image': image, 'label': label}\n",
    "\n",
    "        # add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = label\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(DATA_DIR, \"image_key_cf.csv\")\n",
    "dataset_ours = SAMDataset(data_path, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "image = dataset_ours[index]['pixel_values']\n",
    "label = dataset_ours[index]['ground_truth_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToPILImage()(image)\n",
    "ToPILImage()(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ours_dataloader = DataLoader(dataset_ours, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "epoch_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    batch_epoch_losses = []\n",
    "    for batch in tqdm(train_dataset_ours_dataloader):\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "        \n",
    "        # compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "        \n",
    "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        batch_epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(batch_epoch_losses)}')\n",
    "    epoch_losses.append(mean(batch_epoch_losses))\n",
    "    plt.figure()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
