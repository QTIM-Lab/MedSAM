{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,pdb\n",
    "from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation, SegformerForSemanticSegmentation, SamModel, SamProcessor\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = example['pixel_values']\n",
    "# raw_image = Image.open('/scratch/alpine/skinder@xsede.org/glaucoma_segmentation_data/seg_public_all_vtwo/images/drishtiGS_054.png') # Scott\n",
    "raw_image = Image.open('/sddata/data/MedSAM/public_test_data_01_10_2023/Color_Fundus/visivite_GA.png')\n",
    "# Convert RGBA image to RGB\n",
    "raw_image = raw_image.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(raw_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model = SamModel.from_pretrained(\"flaviagiammarino/medsam-vit-base\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n",
    "\n",
    "# img_url = \"https://huggingface.co/flaviagiammarino/medsam-vit-base/resolve/main/scripts/input.png\"\n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config\n",
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_np = np.array(raw_image)\n",
    "raw_image_np.shape # (1811, 2039, 3)\n",
    "# Define transformations (resize, convert to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1024, 1024)),  # Resize the image and mask to the desired input size\n",
    "    # transforms.Resize((512, 512)),  # Resize the image and mask to the desired input size\n",
    "    transforms.ToTensor(),           # Convert PIL Image to tensor\n",
    "])\n",
    "\n",
    "t_raw_image_np = transform(raw_image)\n",
    "t_raw_image_np = t_raw_image_np.permute(1, 2, 0)\n",
    "print(np.array(raw_image).shape)\n",
    "print(t_raw_image_np.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [600., 600., 1200., 1400.] # [x1, y1, x2, y2] (1811, 2039, 3)\n",
    "# input_boxes = [300., 400., 600., 800.] # [x1, y1, x2, y2] torch.Size([1024, 1024, 3])\n",
    "# input_boxes = [0., 0., 512., 512.]\n",
    "inputs = processor(raw_image, input_boxes=[[input_boxes]], return_tensors=\"pt\").to(device)\n",
    "# inputs = processor(t_raw_image_np, input_boxes=[[input_boxes]], return_tensors=\"pt\").to(device)\n",
    "inputs\n",
    "outputs = model(**inputs, multimask_output=False)\n",
    "# outputs\n",
    "probs = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks.sigmoid().cpu(), \n",
    "    inputs[\"original_sizes\"].cpu(), \n",
    "    inputs[\"reshaped_input_sizes\"].cpu(), binarize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs\n",
    "outputs.pred_masks\n",
    "outputs.keys()\n",
    "print(outputs.pred_masks.shape)\n",
    "print(inputs[\"original_sizes\"])\n",
    "print(inputs[\"reshaped_input_sizes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251/255, 252/255, 30/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=\"blue\", facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(np.array(raw_image))\n",
    "# ax[0].imshow(np.array(t_raw_image_np))\n",
    "show_box(input_boxes, ax[0])\n",
    "ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(np.array(raw_image))\n",
    "# ax[1].imshow(np.array(t_raw_image_np))\n",
    "show_mask(mask=probs[0] > 0.5, ax=ax[1], random_color=False)\n",
    "show_box(input_boxes, ax[1])\n",
    "ax[1].set_title(\"MedSAM Segmentation\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.pred_masks.sigmoid().cpu())\n",
    "print(inputs[\"original_sizes\"].cpu())\n",
    "print(inputs[\"reshaped_input_sizes\"].cpu())\n",
    "probs = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks.sigmoid().cpu(), \n",
    "    inputs[\"original_sizes\"].cpu(), \n",
    "    inputs[\"reshaped_input_sizes\"].cpu(), binarize=False)\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset class\n",
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "model = SamModel.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n",
    "# Modify the model's output layer for segmentation task\n",
    "# For example, for binary segmentation, you can use a sigmoid activation\n",
    "# model.config\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Linear(model.config.vision_config.projection_dim, 1),\n",
    "#     nn.Sigmoid()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "torch.save(model.state_dict(), \"/sddata/projects/MedSAM/MedSamBundle/models/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "model.load_state_dict(torch.load(\"/sddata/projects/MedSAM/MedSamBundle/models/model.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary segmentation\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your dataset and dataloaders\n",
    "train_image = Image.open('/sddata/data/MedSAM/public_test_data_01_10_2023/Color_Fundus/visivite_GA.png')\n",
    "train_mask = Image.open('/sddata/data/MedSAM/public_test_data_01_10_2023/labels/visivite_GA.png')\n",
    "\n",
    "val_image = Image.open('/sddata/data/MedSAM/public_test_data_01_10_2023/Color_Fundus/76_year_old_GA.png')\n",
    "val_mask = Image.open('/sddata/data/MedSAM/public_test_data_01_10_2023/labels/76_year_old_GA.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_np = np.array(train_image)\n",
    "train_mask_np = np.array(train_mask)\n",
    "\n",
    "val_image_np = np.array(val_image)\n",
    "val_mask_np = np.array(val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_image_np.shape)\n",
    "print(train_mask_np.shape)\n",
    "print(val_image_np.shape)\n",
    "print(val_mask_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_np[:,:,0] # R\n",
    "# train_image_np[:,:,1] # G\n",
    "# train_image_np[:,:,2] # B\n",
    "# train_image_np[:,:,3] # alpha\n",
    "\n",
    "# train_image_np[500,500,0] # R pixel at 500, 500\n",
    "# train_image_np[500,500,1]# G pixel at 500, 500\n",
    "# train_image_np[500,500,2]# B pixel at 500, 500\n",
    "# train_image_np[500,500,3]# A pixel at 500, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (resize, convert to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1024, 1024)),  # Resize the image and mask to the desired input size\n",
    "    transforms.ToTensor(),           # Convert PIL Image to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to image and mask\n",
    "train_image = transform(train_image)\n",
    "train_mask = transform(train_mask)\n",
    "train_mask = (train_mask > 0.5).float()\n",
    "# torch.Size([4, 1024, 1024])\n",
    "\n",
    "val_image = transform(val_image)\n",
    "val_mask = transform(val_mask)\n",
    "val_mask = (val_mask > 0.5).float()\n",
    "# torch.Size([4, 1024, 1024])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension to image and mask\n",
    "train_image = train_image.unsqueeze(0)  # Add batch dimension\n",
    "train_mask = train_mask.unsqueeze(0)    # Add batch dimension\n",
    "# torch.Size([1, 4, 1024, 1024])\n",
    "\n",
    "val_image = val_image.unsqueeze(0)  # Add batch dimension\n",
    "val_mask = val_mask.unsqueeze(0)    # Add batch dimension\n",
    "# torch.Size([1, 4, 1024, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_t = train_image[:,0:3,:,:]\n",
    "mask_t = train_mask[:,3,:,:].unsqueeze(1)\n",
    "\n",
    "image_v = val_image[:,0:3,:,:]\n",
    "mask_v = val_mask[:,3,:,:].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes\n",
    "print(\"Train Image shape:\", image_t.shape)  # Should be (1, 3, 224, 224) if RGB image\n",
    "print(\"Train Mask shape:\", mask_t.shape)    # Should be (1, 1, 224, 224) if single-channel mask\n",
    "\n",
    "print(\"Val Image shape:\", image_v.shape)  # Should be (1, 3, 224, 224) if RGB image\n",
    "print(\"Val Mask shape:\", mask_v.shape)    # Should be (1, 1, 224, 224) if single-channel mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageMaskDataset(image_t, mask_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_dataset = ImageMaskDataset(image_v, mask_v)\n",
    "val_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_t.shape\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        pdb.set_trace()\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, masks).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Val Loss: {val_loss}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
